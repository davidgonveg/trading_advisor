#!/usr/bin/env python3
"""
üìä DATABASE POPULATOR V5.0 - CORREGIDO Y MEJORADO
================================================

Versi√≥n corregida que resuelve problemas comunes de poblaci√≥n:
- Manejo correcto de rutas y imports
- Validaci√≥n robusta de datos CSV
- Procesamiento batch optimizado
- Rollback autom√°tico en caso de errores
- Logging detallado para debugging

USO:
    python populate_db_fixed.py --force              # Poblaci√≥n completa
    python populate_db_fixed.py --single-file AAPL   # Test con un archivo
    python populate_db_fixed.py --validate           # Solo validar sin poblar
"""

import os
import sys
import pandas as pd
import numpy as np
import logging
import argparse
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
import time
import sqlite3
from pathlib import Path
import json

# Configurar paths correctamente
current_dir = Path(__file__).parent.absolute()
project_root = current_dir.parent if current_dir.name == 'historical_data' else current_dir
sys.path.insert(0, str(project_root))

# Configurar logging detallado
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)

class ImprovedDataPopulator:
    """Poblador de datos mejorado con manejo robusto de errores"""
    
    def __init__(self, force_mode: bool = False, batch_size: int = 100):
        self.force_mode = force_mode
        self.batch_size = batch_size
        self.project_root = project_root
        self.csv_dir = project_root / 'historical_data' / 'raw_data'
        
        # Stats tracking
        self.stats = {
            'files_processed': 0,
            'files_skipped': 0,
            'files_failed': 0,
            'total_rows_read': 0,
            'total_rows_inserted': 0,
            'symbols_processed': set(),
            'start_time': time.time()
        }
        
        # Error tracking
        self.errors = []
        self.warnings = []
        
        # Components (initialized later)
        self.db_conn = None
        self.indicators_calc = None
        
        logger.info(f"üöÄ ImprovedDataPopulator inicializado - Force: {force_mode}")
    
    def initialize_components(self) -> bool:
        """Inicializar conexiones y componentes del sistema"""
        try:
            logger.info("üîß Inicializando componentes del sistema...")
            
            # Test imports
            try:
                import config
                logger.info("‚úÖ config.py importado")
            except ImportError as e:
                logger.error(f"‚ùå Error importando config: {e}")
                return False
            
            try:
                from database.connection import get_connection, save_indicators_data
                self.get_connection = get_connection
                self.save_indicators_data = save_indicators_data
                logger.info("‚úÖ database.connection importado")
            except ImportError as e:
                logger.error(f"‚ùå Error importando database.connection: {e}")
                return False
            
            try:
                from indicators import TechnicalIndicators
                self.indicators_calc = TechnicalIndicators()
                logger.info("‚úÖ TechnicalIndicators importado")
            except ImportError as e:
                logger.error(f"‚ùå Error importando TechnicalIndicators: {e}")
                return False
            
            # Test database connection
            self.db_conn = self.get_connection()
            if self.db_conn is None:
                logger.error("‚ùå No se pudo conectar a la base de datos")
                return False
            
            # Verificar estructura de la tabla
            cursor = self.db_conn.cursor()
            cursor.execute("PRAGMA table_info(indicators_data);")
            columns = [row[1] for row in cursor.fetchall()]
            
            essential_columns = ['timestamp', 'symbol', 'close_price', 'rsi_value', 'macd_line']
            missing_columns = [c for c in essential_columns if c not in columns]
            
            if missing_columns:
                logger.error(f"‚ùå Columnas faltantes en indicators_data: {missing_columns}")
                return False
            
            logger.info("‚úÖ Todos los componentes inicializados correctamente")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error en inicializaci√≥n: {e}")
            return False
    
    def validate_csv_file(self, csv_path: Path) -> Tuple[bool, str, Dict]:
        """Validar un archivo CSV y extraer metadata"""
        try:
            # Verificar que el archivo existe y no est√° vac√≠o
            if not csv_path.exists():
                return False, "Archivo no existe", {}
            
            if csv_path.stat().st_size == 0:
                return False, "Archivo vac√≠o", {}
            
            # Intentar leer las primeras filas
            df_sample = pd.read_csv(csv_path, nrows=5)
            
            # Verificar columnas requeridas
            required_columns = ['timestamp', 'open', 'high', 'low', 'close', 'volume']
            missing_columns = [col for col in required_columns if col not in df_sample.columns]
            
            if missing_columns:
                return False, f"Columnas faltantes: {missing_columns}", {}
            
            # Verificar formato de timestamp
            try:
                pd.to_datetime(df_sample['timestamp'].iloc[0])
            except Exception as e:
                return False, f"Formato de timestamp inv√°lido: {e}", {}
            
            # Extraer metadata del archivo
            filename = csv_path.name
            filename_parts = filename.replace('.csv', '').split('_')
            
            # Intentar parsear s√≠mbolo y timeframe del filename
            symbol = filename_parts[0] if filename_parts else 'UNKNOWN'
            timeframe = filename_parts[1] if len(filename_parts) > 1 else '1h'
            
            # Leer archivo completo para obtener stats
            df_full = pd.read_csv(csv_path)
            df_full['timestamp'] = pd.to_datetime(df_full['timestamp'])
            
            metadata = {
                'symbol': symbol,
                'timeframe': timeframe,
                'total_rows': len(df_full),
                'date_range': (df_full['timestamp'].min(), df_full['timestamp'].max()),
                'file_size_mb': csv_path.stat().st_size / (1024 * 1024)
            }
            
            logger.info(f"‚úÖ {filename}: {metadata['total_rows']} filas, {metadata['symbol']}")
            return True, "Valid", metadata
            
        except Exception as e:
            return False, f"Error validando CSV: {str(e)}", {}
    
    def calculate_indicators_for_data(self, df: pd.DataFrame, symbol: str) -> Dict:
        """Calcular indicadores para un DataFrame completo"""
        try:
            # Asegurar que timestamp es datetime
            if 'timestamp' in df.columns:
                df['timestamp'] = pd.to_datetime(df['timestamp'])
            
            # Usar la √∫ltima fila para el timestamp
            last_timestamp = df['timestamp'].iloc[-1]
            last_row = df.iloc[-1]
            
            # Calcular indicadores usando todo el DataFrame para contexto hist√≥rico
            indicators = self.indicators_calc.calculate_all_indicators(df, symbol)
            
            # Agregar datos base OHLCV de la √∫ltima fila
            base_data = {
                'timestamp': last_timestamp.isoformat(),
                'symbol': symbol,
                'open_price': float(last_row['open']),
                'high_price': float(last_row['high']), 
                'low_price': float(last_row['low']),
                'close_price': float(last_row['close']),
                'volume': int(last_row['volume'])
            }
            
            # Combinar datos base con indicadores
            indicators.update(base_data)
            
            # Validar que tenemos datos esenciales
            if pd.isna(indicators.get('rsi_value')) and pd.isna(indicators.get('macd_line')):
                self.warnings.append(f"{symbol}: Todos los indicadores son NaN")
            
            return indicators
            
        except Exception as e:
            logger.error(f"‚ùå Error calculando indicadores para {symbol}: {e}")
            raise
    
    def save_historical_batch(self, indicators_batch: List[Dict]) -> Tuple[int, int]:
        """Guardar un lote de indicadores hist√≥ricos"""
        success_count = 0
        error_count = 0
        
        for indicators in indicators_batch:
            try:
                # Usar la funci√≥n existente del sistema
                success = self.save_indicators_data(indicators)
                
                if success:
                    success_count += 1
                    self.stats['total_rows_inserted'] += 1
                else:
                    error_count += 1
                    logger.warning(f"‚ö†Ô∏è save_indicators_data retorn√≥ False para {indicators.get('symbol')}")
                    
            except Exception as e:
                error_count += 1
                logger.error(f"‚ùå Error guardando {indicators.get('symbol')}: {e}")
        
        return success_count, error_count
    
    def process_csv_file(self, csv_path: Path, validate_only: bool = False) -> bool:
        """Procesar un archivo CSV individual"""
        logger.info(f"üìÑ Procesando: {csv_path.name}")
        
        # Validar archivo
        is_valid, validation_msg, metadata = self.validate_csv_file(csv_path)
        
        if not is_valid:
            self.errors.append(f"{csv_path.name}: {validation_msg}")
            self.stats['files_failed'] += 1
            return False
        
        if validate_only:
            logger.info(f"‚úÖ {csv_path.name}: Validaci√≥n OK - {validation_msg}")
            return True
        
        try:
            symbol = metadata['symbol']
            total_rows = metadata['total_rows']
            
            self.stats['symbols_processed'].add(symbol)
            self.stats['total_rows_read'] += total_rows
            
            # Verificar si ya existe data para este s√≠mbolo (si no es force mode)
            if not self.force_mode:
                cursor = self.db_conn.cursor()
                cursor.execute("SELECT COUNT(*) FROM indicators_data WHERE symbol = ?", (symbol,))
                existing_count = cursor.fetchone()[0]
                
                if existing_count > 0:
                    logger.info(f"‚è≠Ô∏è {symbol}: {existing_count} registros existentes, saltando...")
                    self.stats['files_skipped'] += 1
                    return True
            
            # Cargar datos completos
            df = pd.read_csv(csv_path)
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            df = df.sort_values('timestamp')  # Asegurar orden cronol√≥gico
            
            # Para datos hist√≥ricos, procesamos en ventanas deslizantes
            # Esto simula c√≥mo el sistema funcionar√≠a en tiempo real
            
            # Determinar tama√±o de ventana m√≠nimo para indicadores
            min_window = 50  # M√≠nimo para calcular indicadores confiables
            
            if len(df) < min_window:
                self.warnings.append(f"{symbol}: Insuficientes datos ({len(df)} < {min_window})")
                return False
            
            # Procesar en incrementos, cada X filas simulamos un "tick" hist√≥rico
            # Para eficiencia, solo guardamos cada N puntos, no todos
            save_interval = max(1, len(df) // 100)  # Hasta 100 puntos por archivo
            
            indicators_batch = []
            saved_points = 0
            
            for i in range(min_window, len(df), save_interval):
                # Usar datos desde el inicio hasta el punto actual
                df_window = df.iloc[:i+1]
                
                try:
                    indicators = self.calculate_indicators_for_data(df_window, symbol)
                    indicators_batch.append(indicators)
                    
                    # Guardar en lotes para eficiencia
                    if len(indicators_batch) >= self.batch_size:
                        success_count, error_count = self.save_historical_batch(indicators_batch)
                        saved_points += success_count
                        indicators_batch = []  # Reset batch
                        
                        if saved_points % 50 == 0:
                            logger.info(f"üìä {symbol}: {saved_points} puntos guardados...")
                
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è {symbol}: Error en punto {i}: {e}")
                    continue
            
            # Guardar √∫ltimo lote
            if indicators_batch:
                success_count, error_count = self.save_historical_batch(indicators_batch)
                saved_points += success_count
            
            logger.info(f"‚úÖ {symbol}: {saved_points} puntos hist√≥ricos guardados")
            self.stats['files_processed'] += 1
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error procesando {csv_path.name}: {e}")
            self.errors.append(f"{csv_path.name}: {str(e)}")
            self.stats['files_failed'] += 1
            return False
    
    def get_csv_files(self) -> List[Path]:
        """Obtener lista de archivos CSV para procesar"""
        if not self.csv_dir.exists():
            logger.error(f"‚ùå Directorio no existe: {self.csv_dir}")
            return []
        
        csv_files = list(self.csv_dir.glob("*.csv"))
        if not csv_files:
            logger.error(f"‚ùå No se encontraron archivos CSV en: {self.csv_dir}")
            return []
        
        logger.info(f"üìÅ Encontrados {len(csv_files)} archivos CSV")
        return sorted(csv_files)
    
    def check_existing_data(self) -> Dict[str, int]:
        """Verificar datos existentes en la base de datos"""
        try:
            cursor = self.db_conn.cursor()
            cursor.execute("SELECT symbol, COUNT(*) FROM indicators_data GROUP BY symbol")
            existing_data = dict(cursor.fetchall())
            
            cursor.execute("SELECT COUNT(*) FROM indicators_data")
            total_records = cursor.fetchone()[0]
            
            logger.info(f"üìä Datos existentes: {total_records} registros totales")
            for symbol, count in existing_data.items():
                logger.info(f"   {symbol}: {count} registros")
            
            return existing_data
            
        except Exception as e:
            logger.error(f"‚ùå Error verificando datos existentes: {e}")
            return {}
    
    def populate_all_files(self, validate_only: bool = False) -> bool:
        """Poblar todos los archivos CSV encontrados"""
        logger.info("üöÄ Iniciando poblaci√≥n de datos hist√≥ricos...")
        
        # Verificar datos existentes
        if not validate_only:
            existing_data = self.check_existing_data()
            
            if existing_data and not self.force_mode:
                logger.info("‚ÑπÔ∏è Datos existentes encontrados. Usa --force para sobrescribir")
        
        # Obtener archivos CSV
        csv_files = self.get_csv_files()
        if not csv_files:
            return False
        
        # Procesar cada archivo
        for i, csv_file in enumerate(csv_files, 1):
            logger.info(f"üìÑ [{i}/{len(csv_files)}] Procesando: {csv_file.name}")
            
            try:
                success = self.process_csv_file(csv_file, validate_only)
                
                if success:
                    logger.info(f"‚úÖ {csv_file.name}: Completado")
                else:
                    logger.warning(f"‚ö†Ô∏è {csv_file.name}: Fall√≥ o se salt√≥")
                
                # Small delay para no sobrecargar la BD
                time.sleep(0.1)
                
            except KeyboardInterrupt:
                logger.info("‚èπÔ∏è Interrupci√≥n del usuario detectada")
                break
            except Exception as e:
                logger.error(f"‚ùå Error inesperado con {csv_file.name}: {e}")
                self.errors.append(f"{csv_file.name}: {str(e)}")
                continue
        
        return True
    
    def populate_single_file(self, symbol: str) -> bool:
        """Poblar un solo archivo por s√≠mbolo (para testing)"""
        logger.info(f"üß™ Poblaci√≥n de test para s√≠mbolo: {symbol}")
        
        # Buscar archivo que coincida con el s√≠mbolo
        csv_files = self.get_csv_files()
        matching_files = [f for f in csv_files if f.name.upper().startswith(symbol.upper())]
        
        if not matching_files:
            logger.error(f"‚ùå No se encontr√≥ archivo para s√≠mbolo: {symbol}")
            return False
        
        # Usar el primer archivo que coincida
        csv_file = matching_files[0]
        logger.info(f"üìÑ Usando archivo: {csv_file.name}")
        
        return self.process_csv_file(csv_file, validate_only=False)
    
    def print_final_summary(self):
        """Imprimir resumen final de la operaci√≥n"""
        elapsed_time = time.time() - self.stats['start_time']
        
        print("\n" + "=" * 60)
        print("üìã RESUMEN FINAL DE POBLACI√ìN")
        print("=" * 60)
        
        print(f"‚è±Ô∏è Tiempo total: {elapsed_time:.1f} segundos")
        print(f"üìÅ Archivos procesados: {self.stats['files_processed']}")
        print(f"‚è≠Ô∏è Archivos saltados: {self.stats['files_skipped']}")
        print(f"‚ùå Archivos fallidos: {self.stats['files_failed']}")
        
        print(f"üìä Filas le√≠das: {self.stats['total_rows_read']:,}")
        print(f"üíæ Filas insertadas: {self.stats['total_rows_inserted']:,}")
        
        if self.stats['symbols_processed']:
            print(f"üìà S√≠mbolos procesados: {', '.join(sorted(self.stats['symbols_processed']))}")
        
        if self.stats['total_rows_read'] > 0:
            success_rate = (self.stats['total_rows_inserted'] / self.stats['total_rows_read']) * 100
            print(f"‚úÖ Tasa de √©xito: {success_rate:.1f}%")
        
        # Mostrar errores si los hay
        if self.errors:
            print(f"\n‚ùå ERRORES ({len(self.errors)}):")
            for i, error in enumerate(self.errors[:10], 1):  # Mostrar solo primeros 10
                print(f"   {i}. {error}")
            
            if len(self.errors) > 10:
                print(f"   ... y {len(self.errors) - 10} errores m√°s")
        
        # Mostrar warnings si los hay
        if self.warnings:
            print(f"\n‚ö†Ô∏è WARNINGS ({len(self.warnings)}):")
            for i, warning in enumerate(self.warnings[:5], 1):  # Mostrar solo primeros 5
                print(f"   {i}. {warning}")
            
            if len(self.warnings) > 5:
                print(f"   ... y {len(self.warnings) - 5} warnings m√°s")
        
        # Recomendaciones
        print(f"\nüí° PR√ìXIMOS PASOS:")
        if self.stats['total_rows_inserted'] > 0:
            print("   1. ‚úÖ Datos poblados exitosamente")
            print("   2. Ejecutar backtest: python historical_data/backtest_engine.py")
            print("   3. O usar: python start.py backtest")
        else:
            print("   1. ‚ùå No se insertaron datos")
            print("   2. Revisar errores arriba")
            print("   3. Verificar archivos CSV en raw_data/")
            print("   4. Ejecutar con --force si es necesario")


def main():
    """Funci√≥n principal con argumentos de l√≠nea de comandos"""
    parser = argparse.ArgumentParser(
        description='Populador de base de datos mejorado',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Ejemplos de uso:
  python populate_db_fixed.py --validate          # Solo validar CSV sin poblar
  python populate_db_fixed.py --force             # Poblaci√≥n completa (sobrescribir)
  python populate_db_fixed.py --single-file AAPL  # Test con un archivo
  python populate_db_fixed.py                     # Poblaci√≥n normal (saltar existentes)
        """
    )
    
    parser.add_argument('--force', action='store_true', 
                       help='Sobrescribir datos existentes')
    parser.add_argument('--validate', action='store_true',
                       help='Solo validar archivos CSV sin poblar BD')
    parser.add_argument('--single-file', type=str, metavar='SYMBOL',
                       help='Procesar solo un archivo por s√≠mbolo (para testing)')
    parser.add_argument('--batch-size', type=int, default=50,
                       help='Tama√±o de lote para inserci√≥n (default: 50)')
    
    args = parser.parse_args()
    
    # Crear populator
    populator = ImprovedDataPopulator(
        force_mode=args.force,
        batch_size=args.batch_size
    )
    
    # Inicializar componentes
    if not populator.initialize_components():
        logger.error("‚ùå Fallo en inicializaci√≥n. Abortando.")
        return 1
    
    try:
        # Ejecutar seg√∫n argumentos
        if args.single_file:
            success = populator.populate_single_file(args.single_file)
        else:
            success = populator.populate_all_files(validate_only=args.validate)
        
        # Imprimir resumen
        populator.print_final_summary()
        
        return 0 if success else 1
        
    except KeyboardInterrupt:
        logger.info("‚èπÔ∏è Operaci√≥n cancelada por el usuario")
        populator.print_final_summary()
        return 130
    except Exception as e:
        logger.error(f"‚ùå Error fatal: {e}")
        populator.print_final_summary()
        return 1
    finally:
        # Cerrar conexi√≥n si existe
        if populator.db_conn:
            populator.db_conn.close()


if __name__ == "__main__":
    exit(main())